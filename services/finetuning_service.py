import os
import torch
import logging
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from dotenv import load_dotenv

logger = logging.getLogger(__name__)
load_dotenv(dotenv_path=os.path.join(os.path.dirname(__file__), '..', '.env'))

class FinetuningService:
    """íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ ì‚¬ìš©í•œ ë£° ì„¤ëª… ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        logger.info("ğŸ”§ íŒŒì¸íŠœë‹ ì„œë¹„ìŠ¤ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")
        
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"ğŸ’» ë””ë°”ì´ìŠ¤: {self.device}")
        
        # ëª¨ë¸ ë¡œë“œ
        self._load_model()
        
        logger.info("âœ… íŒŒì¸íŠœë‹ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _load_model(self):
        """íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ë¡œë“œ (base ëª¨ë¸ + PEFT ì–´ëŒ‘í„°)"""
        try:
            finetuned_model_name = os.getenv("FINETUNING_MODEL_ID")
            logger.info(f"ğŸ“¥ íŒŒì¸íŠœë‹ëœ ì–´ëŒ‘í„° ë¡œë“œ ì¤‘: {finetuned_model_name}")
            
            # 1. base ëª¨ë¸ ëª… ì§€ì • (í™˜ê²½ë³€ìˆ˜ or ì½”ë“œ ë‚´ ê¸°ë³¸ê°’)
            base_model_name = "beomi/KoAlpaca-Polyglot-5.8B"
            logger.info(f"ğŸ“¥ base ëª¨ë¸ ë¡œë“œ ì¤‘: {base_model_name}")
            
            # 2. base ëª¨ë¸ ë¡œë“œ
            base_model = AutoModelForCausalLM.from_pretrained(
                base_model_name,
                device_map="auto",
                torch_dtype=torch.float16,
                trust_remote_code=True
            )
            
            # 3. adapter(PEFT) ëª¨ë¸ ë¡œë“œí•˜ì—¬ base ëª¨ë¸ì— ì”Œìš°ê¸°
            self.model = PeftModel.from_pretrained(
                base_model,
                finetuned_model_name,
                device_map="auto",
                torch_dtype=torch.float16,
                trust_remote_code=True
            )
            
            # 4. tokenizer ë¡œë“œ (ê¸°ë³¸ì ìœ¼ë¡œ ì–´ëŒ‘í„°ê°€ ì•„ë‹ˆë¼ base ëª¨ë¸ tokenizer ì‚¬ìš©)
            logger.info("ğŸ“¥ Tokenizer ë¡œë“œ ì¤‘...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                base_model_name,
                use_fast=False,
                trust_remote_code=True
            )
            
            logger.info("âœ… íŒŒì¸íŠœë‹ ëª¨ë¸(ì–´ëŒ‘í„°) ë¡œë“œ ì™„ë£Œ")
                
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            logger.info("ğŸ”„ ê¸°ë³¸ ëª¨ë¸ë¡œ ëŒ€ì²´ ì‹œë„...")
            
            try:
                base_model_name = "beomi/KoAlpaca-Polyglot-5.8B"
                logger.info(f"ğŸ“¥ ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ: {base_model_name}")
                
                self.model = AutoModelForCausalLM.from_pretrained(
                    base_model_name,
                    device_map="auto",
                    torch_dtype=torch.float16,
                    trust_remote_code=True
                )
                
                self.tokenizer = AutoTokenizer.from_pretrained(
                    base_model_name, 
                    use_fast=False,
                    trust_remote_code=True
                )
                
                logger.info("âœ… ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                
            except Exception as backup_e:
                logger.error(f"âŒ ê¸°ë³¸ ëª¨ë¸ ë¡œë“œë„ ì‹¤íŒ¨: {str(backup_e)}")
                self.model = None
                self.tokenizer = None
    
    async def answer_question(self, game_name: str, question: str):
        """íŒŒì¸íŠœë‹ëœ ëª¨ë¸ë¡œ ì§ˆë¬¸ ë‹µë³€"""
        try:
            if not self.model or not self.tokenizer:
                return "íŒŒì¸íŠœë‹ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            
            prompt = f"ì´ ì§ˆë¬¸ì€ '{game_name}'ì´ë¼ëŠ” ë³´ë“œê²Œì„ì— ëŒ€í•œ ê²ƒì´ë‹¤.\n### ì§ˆë¬¸: {question}"
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
            inputs.pop("token_type_ids", None)
            
            with torch.no_grad():
                output = self.model.generate(
                    **inputs,
                    max_new_tokens=128,
                    do_sample=False,
                    temperature=0.0,
                    top_p=1.0,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            result = self.tokenizer.decode(output[0], skip_special_tokens=True)
            answer = result.replace(prompt, "").strip()
            
            if not answer:
                answer = f"'{game_name}' ê²Œì„ì— ëŒ€í•œ '{question}' ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            return answer
            
        except Exception as e:
            logger.error(f"âŒ íŒŒì¸íŠœë‹ ëª¨ë¸ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return f"íŒŒì¸íŠœë‹ ëª¨ë¸ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def get_model_info(self):
        return {
            "model_loaded": self.model is not None,
            "tokenizer_loaded": self.tokenizer is not None,
            "device": self.device,
            "finetuned_model": os.getenv("FINETUNING_MODEL_ID"),
            "base_model": os.getenv("BASE_MODEL_ID", "beomi/KoAlpaca-Polyglot-5.8B")
        }
