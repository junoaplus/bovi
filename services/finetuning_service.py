import os
import torch
import logging
from transformers import AutoTokenizer, AutoModelForCausalLM

logger = logging.getLogger(__name__)

class FinetuningService:
    """νμΈνλ‹λ λ¨λΈμ„ μ‚¬μ©ν• λ£° μ„¤λ… μ„λΉ„μ¤"""
    
    def __init__(self):
        logger.info("π”§ νμΈνλ‹ μ„λΉ„μ¤λ¥Ό μ΄κΈ°ν™”ν•©λ‹λ‹¤...")
        
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"π’» λ””λ°”μ΄μ¤: {self.device}")
        
        # λ¨λΈ λ΅λ“
        self._load_model()
        
        logger.info("β… νμΈνλ‹ μ„λΉ„μ¤ μ΄κΈ°ν™” μ™„λ£")
    
    def _load_model(self):
        """νμΈνλ‹λ λ¨λΈ λ΅λ“"""
        try:
            # ν—κΉ…νμ΄μ¤μ—μ„ μ§μ ‘ νμΈνλ‹λ λ¨λΈ λ΅λ“
            finetuned_model_name = "minjeongHuggingFace/koalpaca-bang_e9"
            logger.info(f"π“¥ νμΈνλ‹λ λ¨λΈ λ΅λ“ μ¤‘: {finetuned_model_name}")
            
            # 1. νμΈνλ‹λ λ¨λΈ λ΅λ“
            self.model = AutoModelForCausalLM.from_pretrained(
                finetuned_model_name,
                device_map="auto",
                torch_dtype=torch.float16,
                trust_remote_code=True
            )
            
            # 2. Tokenizer λ΅λ“ (κ°™μ€ λ¨λΈμ—μ„)
            logger.info("π“¥ Tokenizer λ΅λ“ μ¤‘...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                finetuned_model_name, 
                use_fast=False,
                trust_remote_code=True
            )
            
            logger.info("β… νμΈνλ‹ λ¨λΈ λ΅λ“ μ™„λ£")
                
        except Exception as e:
            logger.error(f"β λ¨λΈ λ΅λ“ μ‹¤ν¨: {str(e)}")
            logger.info("π”„ κΈ°λ³Έ λ¨λΈλ΅ λ€μ²΄ μ‹λ„...")
            
            try:
                # κΈ°λ³Έ KoAlpaca λ¨λΈλ΅ λ€μ²΄
                base_model_name = "beomi/KoAlpaca-Polyglot-5.8B"
                logger.info(f"π“¥ κΈ°λ³Έ λ¨λΈ λ΅λ“: {base_model_name}")
                
                self.model = AutoModelForCausalLM.from_pretrained(
                    base_model_name,
                    device_map="auto",
                    torch_dtype=torch.float16,
                    trust_remote_code=True
                )
                
                self.tokenizer = AutoTokenizer.from_pretrained(
                    base_model_name, 
                    use_fast=False,
                    trust_remote_code=True
                )
                
                logger.info("β… κΈ°λ³Έ λ¨λΈ λ΅λ“ μ™„λ£")
                
            except Exception as backup_e:
                logger.error(f"β κΈ°λ³Έ λ¨λΈ λ΅λ“λ„ μ‹¤ν¨: {str(backup_e)}")
                # λ¨λΈ λ΅λ“ μ‹¤ν¨ μ‹ NoneμΌλ΅ μ„¤μ •
                self.model = None
                self.tokenizer = None
    
    async def answer_question(self, game_name: str, question: str):
        """νμΈνλ‹λ λ¨λΈλ΅ μ§λ¬Έ λ‹µλ³€"""
        try:
            if not self.model or not self.tokenizer:
                return "νμΈνλ‹ λ¨λΈμ΄ λ΅λ“λμ§€ μ•μ•μµλ‹λ‹¤."
            
            # ν”„λ΅¬ν”„νΈ μƒμ„±
            prompt = f"μ΄ μ§λ¬Έμ€ '{game_name}'μ΄λΌλ” λ³΄λ“κ²μ„μ— λ€ν• κ²ƒμ΄λ‹¤.\n### μ§λ¬Έ: {question}"
            
            # ν† ν°ν™”
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
            
            # token_type_idsκ°€ μμΌλ©΄ μ κ±° (μΌλ¶€ λ¨λΈμ—μ„ μ—λ¬ λ°©μ§€)
            inputs.pop("token_type_ids", None)
            
            # μ¶”λ΅ 
            with torch.no_grad():
                output = self.model.generate(
                    **inputs,
                    max_new_tokens=128,
                    do_sample=False,    # μƒν”λ§ OFF
                    temperature=0.0,    # μ™„μ „ λ‚®μ¶¤
                    top_p=1.0,          # μ „λ¶€ ν—μ©
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            # κ²°κ³Ό λ””μ½”λ”©
            result = self.tokenizer.decode(output[0], skip_special_tokens=True)
            
            # ν”„λ΅¬ν”„νΈ λ¶€λ¶„ μ κ±°ν•κ³  λ‹µλ³€λ§ μ¶”μ¶
            answer = result.replace(prompt, "").strip()
            
            # λ‹µλ³€μ΄ λΉ„μ–΄μμΌλ©΄ κΈ°λ³Έ λ©”μ‹μ§€
            if not answer:
                answer = f"'{game_name}' κ²μ„μ— λ€ν• '{question}' μ§λ¬Έμ— λ€ν• λ‹µλ³€μ„ μƒμ„±ν•  μ μ—†μµλ‹λ‹¤."
            
            return answer
            
        except Exception as e:
            logger.error(f"β νμΈνλ‹ λ¨λΈ λ‹µλ³€ μƒμ„± μ‹¤ν¨: {str(e)}")
            return f"νμΈνλ‹ λ¨λΈ λ‹µλ³€ μƒμ„± μ¤‘ μ¤λ¥κ°€ λ°μƒν–μµλ‹λ‹¤: {str(e)}"
    
    def get_model_info(self):
        """λ¨λΈ μ •λ³΄ λ°ν™"""
        return {
            "model_loaded": self.model is not None,
            "tokenizer_loaded": self.tokenizer is not None,
            "device": self.device,
            "finetuned_model": "minjeongHuggingFace/koalpaca-bang_e9",
            "fallback_model": "beomi/KoAlpaca-Polyglot-5.8B"
        }
