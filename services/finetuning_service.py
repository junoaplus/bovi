import os
import torch
import logging
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

logger = logging.getLogger(__name__)

class FinetuningService:
    """νμΈνλ‹λ λ¨λΈμ„ μ‚¬μ©ν• λ£° μ„¤λ… μ„λΉ„μ¤"""
    
    def __init__(self):
        logger.info("π”§ νμΈνλ‹ μ„λΉ„μ¤λ¥Ό μ΄κΈ°ν™”ν•©λ‹λ‹¤...")
        
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"π’» λ””λ°”μ΄μ¤: {self.device}")
        
        # λ¨λΈ λ΅λ“
        self._load_model()
        
        logger.info("β… νμΈνλ‹ μ„λΉ„μ¤ μ΄κΈ°ν™” μ™„λ£")
    
    def _load_model(self):
        """νμΈνλ‹λ λ¨λΈ λ΅λ“"""
        try:
            # 1. Base λ¨λΈ λ΅λ“
            base_model_name = "beomi/KoAlpaca-Polyglot-5.8B"
            logger.info(f"π“¥ Base λ¨λΈ λ΅λ“ μ¤‘: {base_model_name}")
            
            self.base_model = AutoModelForCausalLM.from_pretrained(
                base_model_name,
                device_map="auto",
                torch_dtype=torch.float16,
                trust_remote_code=True
            )
            
            # 2. Tokenizer λ΅λ“
            logger.info("π“¥ Tokenizer λ΅λ“ μ¤‘...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                base_model_name, 
                use_fast=False,
                trust_remote_code=True
            )
            
            # 3. LoRA μ–΄λ‘ν„° λ΅λ“
            lora_model_path = "./models/koalpaca-bang-model"
            if os.path.exists(lora_model_path):
                logger.info(f"π“¥ LoRA λ¨λΈ λ΅λ“ μ¤‘: {lora_model_path}")
                self.model = PeftModel.from_pretrained(self.base_model, lora_model_path)
                
                # 4. LoRA λ³‘ν•© (μ„ νƒμ‚¬ν•­)
                logger.info("π”— LoRA μ–΄λ‘ν„° λ³‘ν•© μ¤‘...")
                self.model = self.model.merge_and_unload()
                
                logger.info("β… νμΈνλ‹ λ¨λΈ λ΅λ“ μ™„λ£")
            else:
                logger.warning(f"β οΈ LoRA λ¨λΈμ„ μ°Ύμ„ μ μ—†μ–΄ Base λ¨λΈλ§ μ‚¬μ©ν•©λ‹λ‹¤: {lora_model_path}")
                self.model = self.base_model
                
        except Exception as e:
            logger.error(f"β λ¨λΈ λ΅λ“ μ‹¤ν¨: {str(e)}")
            # λ¨λΈ λ΅λ“ μ‹¤ν¨ μ‹ NoneμΌλ΅ μ„¤μ •
            self.model = None
            self.tokenizer = None
    
    async def answer_question(self, game_name: str, question: str):
        """νμΈνλ‹λ λ¨λΈλ΅ μ§λ¬Έ λ‹µλ³€"""
        try:
            if not self.model or not self.tokenizer:
                return "νμΈνλ‹ λ¨λΈμ΄ λ΅λ“λμ§€ μ•μ•μµλ‹λ‹¤."
            
            # ν”„λ΅¬ν”„νΈ μƒμ„±
            prompt = f"μ΄ μ§λ¬Έμ€ '{game_name}'μ΄λΌλ” λ³΄λ“κ²μ„μ— λ€ν• κ²ƒμ΄λ‹¤.\n### μ§λ¬Έ: {question}"
            
            # ν† ν°ν™”
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
            
            # token_type_idsκ°€ μμΌλ©΄ μ κ±° (μΌλ¶€ λ¨λΈμ—μ„ μ—λ¬ λ°©μ§€)
            inputs.pop("token_type_ids", None)
            
            # μ¶”λ΅ 
            with torch.no_grad():
                output = self.model.generate(
                    **inputs,
                    max_new_tokens=128,
                    do_sample=False,    # μƒν”λ§ OFF
                    temperature=0.0,    # μ™„μ „ λ‚®μ¶¤
                    top_p=1.0,          # μ „λ¶€ ν—μ©
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            # κ²°κ³Ό λ””μ½”λ”©
            result = self.tokenizer.decode(output[0], skip_special_tokens=True)
            
            # ν”„λ΅¬ν”„νΈ λ¶€λ¶„ μ κ±°ν•κ³  λ‹µλ³€λ§ μ¶”μ¶
            answer = result.replace(prompt, "").strip()
            
            # λ‹µλ³€μ΄ λΉ„μ–΄μμΌλ©΄ κΈ°λ³Έ λ©”μ‹μ§€
            if not answer:
                answer = f"'{game_name}' κ²μ„μ— λ€ν• '{question}' μ§λ¬Έμ— λ€ν• λ‹µλ³€μ„ μƒμ„±ν•  μ μ—†μµλ‹λ‹¤."
            
            return answer
            
        except Exception as e:
            logger.error(f"β νμΈνλ‹ λ¨λΈ λ‹µλ³€ μƒμ„± μ‹¤ν¨: {str(e)}")
            return f"νμΈνλ‹ λ¨λΈ λ‹µλ³€ μƒμ„± μ¤‘ μ¤λ¥κ°€ λ°μƒν–μµλ‹λ‹¤: {str(e)}"
    
    def get_model_info(self):
        """λ¨λΈ μ •λ³΄ λ°ν™"""
        return {
            "model_loaded": self.model is not None,
            "tokenizer_loaded": self.tokenizer is not None,
            "device": self.device,
            "base_model": "beomi/KoAlpaca-Polyglot-5.8B",
            "lora_model": "./models/koalpaca-bang-model"
        }
